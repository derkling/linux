			   =======================
			   Energy Aware Scheduling
			   =======================

1. Introduction
---------------

Energy Aware Scheduling (or EAS) gives the ability to the scheduler to predict
the impact of its decisions on the system energy. EAS relies on an Energy Model
(EM) of the platform to select an energy efficient CPU for each task, with a
minimal impact on throughput. This document aims at providing an introduction
on how EAS works, what are the main design decisions behind it, and detail what
is needed to get it to run.

Before going any further, please note that at the time of writing:

   /!\ EAS does not support platforms with symmetric CPU topologies /!\

For now, EAS operates only on platforms with heterogeneous CPU topologies (such
as Arm big.LITTLE) because this is where the potential for saving energy through
scheduling is the highest.

The actual EM used by EAS is _not_ maintained by the scheduler, but by a
dedicated framework. For details about this framework and what it provides,
please refer to its documentation, not to the scheduler's.


2. Background and Terminology
-----------------------------

To make it clear from the start:
 - energy = [joule] (resource like a battery on powered devices)
 - power = energy/time = [joule/second] = [watt]

The goal of EAS is to minimize energy, while still getting the job done. That is,
we want to maximize:

	performance [inst/s]
	--------------------
	    power [W]

which is equivalent to minimizing:

	energy [J]
	-----------
	instruction

while still getting 'good' performance. It is essentially an alternative
optimization objective to the current performance-only objective for the
scheduler. This alternative considers two objectives: energy-efficiency and
performance.

The idea behind introducing an energy cost model is to allow the scheduler to
evaluate the implications of its decisions rather than applying energy-saving
techniques blindly that may only have positive effects on some platforms. At
the same time, the energy cost model must be as simple as possible to minimize
the scheduler latency impact.


3. Topology representation
--------------------------

Most of the platform knowledge used by EAS is directly read from the EM
framework which basically provides a power cost table and a cpumask for each
'performance domain' in the system. A performance domain is a group of CPUs
whose performance is scaled together. All the CPUs of a performance domain must
have the same micro-architecture. Performance domains generally have a 1-to-1
mapping with CPUFreq policies.

The scheduler manages references to the EM objects in the topology code when the
scheduling domains are built, or re-built. EAS maintains for each root domain
(rd) a singly linked list of all performance domains intersecting the rd->span.
Each node in the list contains a pointer to a struct em_perf_domain as provided
by the EM framework.

The lists are attached to the root domains in order to cope with complex
scenarios including exclusive cpuset configurations. Since the boundaries of
exclusive cpusets do not not necessarily match those of performance domains, the
lists of different root domains can contain duplicate elements.

Example 2.
    Let us consider a platform with 12 CPUs, split in 3 performance domains
    (pd0, pd4 and pd8), organized as follows:

	          CPUs:   0 1 2 3 4 5 6 7 8 9 10 11
	          PDs:   |--pd0--|--pd4--|---pd8---|
	          RDs:   |----rd1----|-----rd2-----|

    Now, consider that userspace decided to split the system with two
    exclusive cpusets, hence creating two independent root domains, each
    containing 6 CPUs. The two root domains are denoted rd1 and rd2 in the
    above figure. Since pd4 intersects with both rd1 and rd2, it will be
    present in the linked list '->pd' attached to each of them:
       * rd1->pd: pd0 -> pd4
       * rd2->pd: pd4 -> pd8

    Please note that the scheduler will create two duplicate list nodes for
    pd4 (one for each list). However, both just hold a pointer to the same
    shared data structure of the EM framework.

Since the access to these lists can happen concurrently with hotplug and other
things, they are protected by RCU, like the rest of topology structures
manipulated by the scheduler.

EAS also maintains a static key (sched_energy_present) which is enabled when at
least one root domain meets all conditions for EAS to start. Those conditions
are summarized in Section 6.


4. Energy-Aware task placement
------------------------------

EAS overloads the CFS task wake-up balancing code. It uses the EM of the
platform and the task's PELT signals to choose an energy-efficient target CPU
during wake-up balance. When EAS is enabled, select_task_rq_fair() calls
find_energy_efficient_cpu() (abbreviated feec() in the rest of the document) to
do the placement decision. feec() looks for the CPU with the highest spare
capacity in each performance domain, and checks if placing the task there could
save energy compared to leaving it on prev_cpu.

feec() uses compute_energy() to estimate what will be the energy consumed by the
system if the waking task was migrated. compute_energy() looks at the current
utilization landscape of the CPUs and adjusts it to 'simulate' the task
migration. The EM framework provides the em_pd_energy() API which computes the
expected energy consumption of each performance domain for the given utilization
landscape.

An example of energy-optimized task placement decision is detailed below.

Example 1.
    Let us consider a (fake) platform with an asymmetric CPU topology. The
    platform has 2 independent performance domains for CPUs, with two CPUs
    in each. CPU0 and CPU1 are little CPUs; CPU2 and CPU3 big.

    The scheduler must decide where to place a task P with 200 of util_avg.
    Its prev_cpu is CPU 0.

    The current utilization landscape of the CPUs is depicted on the graph
    below. CPUs 0 to 3 respectively have 200, 100, 600 and 500 of current
    utilization (as given by cpu_util_wake(*, P), meaning that the util_avg
    of P has been removed). Each performance domain has three Operating
    Performance Points (OPPs). The CPU capacity and power cost associated
    with each OPP is listed in the Energy Model table.

    CPU util.
      1024                 - - - - - - -              Energy Model
                                               +-----------+-------------+
                                               |  Little   |     Big     |
       768                 =============       +-----+-----+------+------+
                                               | Cap | Pwr | Cap  | Pwr  |
                                               +-----+-----+------+------+
       512  - - - - - -    - ##- - - - -       | 170 | 50  | 512  | 400  |
                             ##     ##         | 341 | 150 | 768  | 800  |
       341  ===========      ##     ##         | 512 | 300 | 1024 | 1700 |
                             ##     ##         +-----+-----+------+------+
       170  -## - - - -      ##     ##
             ##     ##       ##     ##
           ------------    -------------
            CPU0   CPU1     CPU2   CPU3

      Current OPP: =====       Other OPP: - - -      100 of util_avg: ##


    feec() will first look for the CPUs with the maximum spare capacity
    in the two performance domains. In this example, CPU1 and CPU3. Then
    it will estimate the energy of the system if P was placed on either
    of them, and check if that would save some energy compared to leaving
    P on CPU0. EAS assumes that OPPs follow utilization (which is coherent
    with the behaviour of the schedutil CPUFreq governor, see Section 6.
    for more details on this topic).

    The util_avg of P is shown on the figures below as 'PP'.

    Case 1. P is migrated to CPU1
    *****************************

      1024                 - - - - - - -

                                            Energy calculation:
       768                 =============     * CPU0: 200 / 341 * 150 = 88
                                             * CPU1: 300 / 341 * 150 = 131
                                             * CPU2: 600 / 768 * 800 = 625
       512  - - - - - -    - ##- - - - -     * CPU3: 500 / 768 * 800 = 520
                             ##     ##          => total_energy = 1364
       341  ===========      ##     ##
                    PP       ##     ##
       170  -## - - PP-      ##     ##
             ##     ##       ##     ##
           ------------    -------------
            CPU0   CPU1     CPU2   CPU3


    Case 2. P is migrated to CPU3
   ******************************

      1024                 - - - - - - -

                                            Energy calculation:
       768                 =============     * CPU0: 200 / 341 * 150 = 88
                                             * CPU1: 100 / 341 * 150 = 43
                                    PP       * CPU2: 600 / 768 * 800 = 625
       512  - - - - - -    - ##- - -PP -     * CPU3: 700 / 768 * 800 = 729
                             ##     ##          => total_energy = 1485
       341  ===========      ##     ##
                             ##     ##
       170  -## - - - -      ##     ##
             ##     ##       ##     ##
           ------------    -------------
            CPU0   CPU1     CPU2   CPU3


    Case 3. P stays on prev_cpu / CPU 0
    ***********************************

      1024                 - - - - - - -

                                            Energy calculation:
       768                 =============     * CPU0: 400 / 512 * 300 = 234
                                             * CPU1: 100 / 512 * 300 = 58
                                             * CPU2: 600 / 768 * 800 = 625
       512  ===========    - ##- - - - -     * CPU3: 500 / 768 * 800 = 520
                             ##     ##          => total_energy = 1437
       341  -PP - - - -      ##     ##
             PP              ##     ##
       170  -## - - - -      ##     ##
             ##     ##       ##     ##
           ------------    -------------
            CPU0   CPU1     CPU2   CPU3


    From these calculations, the Case 1 has the lowest total energy. So CPU 1
    is be the best candidate from an energy-efficiency standpoint.


One may wonder, given Example 1, if it ever makes sense to use the big cores for
energy reasons. The answer is yes. That can be the case in various scenarios
depending on the platforms. For some systems, the high OPPs of the little CPUs
can be less energy efficient than the lowest OPPs of the bigs. So, if the little
CPUs happen to have enough load, a small task could be better of executing on
the big side in order to save energy.

And even in the case where all OPPs of the big CPUs are less energy-efficient
than those of the little, using the big CPUs for a small task might still, under
specific conditions, save energy. Placing a task on a little CPU can result
in raising the OPP of the entire cluster, and that will increase the cost of
the tasks already running there. If the waking task is placed on a big CPU, its
own execution cost might be higher than if it was running on a little, but it
won't impact the other tasks of the little CPUs which will keep running at a
lower OPP. So, at the system level, the extra cost of running that one task on a
big core can be smaller than the cost of raising the OPP on the little CPUs for
all other tasks.

The examples above would be nearly impossible to get right in a generic way, and
for all platforms, without knowing the cost of running at different OPPs on all
CPUs of the system. Thanks to its EM-based design, EAS should cope with them
correctly without too many troubles. However, in order to ensure a minimal
impact on throughput for high-load scenarios, EAS implements another mechanism
called 'over-utilization'.


5. Over-utilization
-------------------

From a general standpoint, the use-cases where EAS can help the most are those
involving a light/medium CPU load. Whenever long CPU-bound tasks are being run,
they will require all of the available CPU bandwidth, and there isn't much that
can be done from the scheduler to save energy without harming throughput
severely. In order to avoid hurting performance with EAS, CPUs are flagged as
'over-utilized' as soon as they are used at more than 80% of their compute
capacity. As long as no CPUs are over-utilized in a root domain, periodic load
balancing is disabled hence giving EAS all the freedom to safely balance things
out during task wake-up using only the utilization signals of tasks and CPUs.
It is safe to do so when the system isn't overutilized since being below the
80% tipping point implies that:

    a. there is some idle time on all CPUs, so the utilization signals are
       likely to accurately represent the 'size' of the various tasks in the
       system;
    b. all tasks should already be provided with enough CPU bandwidth,
       regardless of their nice values.

As soon as one CPU goes above the 80% tipping point, at least one of the two
above assumptions can become incorrect. So, the overutilized flag is raised for
the entire root domain and EAS is disabled there. The scheduler falls back onto
the classic load-based balancing code for both wake-up and periodic load balance
which guarantees a better respect of the nice values.

Since the notion of overutilization largely relies on detecting whether or not
there is some idle time in the system, the CPU capacity used by higher (than
CFS) scheduling classes must be taken into account. As such, the 80% tipping
point is computed against the CPU capacity reduced by scale_rt_capacity().


6. Dependencies and requirements for EAS
----------------------------------------

Energy Aware Scheduling depends on the CPUs of the system having specific
hardware properties and on other features of the kernel being enabled. This
section is an attempt to list these dependencies and to provide hints as to how
they can be met.


  6.1 - Asymmetric CPU topology

As mentioned in the introduction, EAS is only supported on platforms with
asymmetric CPU topologies for now. This requirement is checked at run-time by
looking for the presence of the SD_ASYM_CPUCAPACITY flag when the scheduling
domains are built.

The flag is set/cleared automatically by the scheduler topology code whenever
there are CPUs with different capacities in a root domain. The capacities of
CPUs are provided by arch-specific code through the arch_scale_cpu_capacity()
callback. As an example, arm and arm64 share an implementation of this callback
which uses a combination of CPUFreq data and device-tree bindings to compute the
capacity of CPUs (see drivers/base/arch_topology.c for more details).

So, in order to use EAS on your platform your architecture must implement the
arch_scale_cpu_capacity() callback, and some of the CPUs must have a lower
capacity than others.


  6.2 - Energy Model presence

EAS uses the Energy Model of a platform to estimate the impact of scheduling
decisions on energy. So, your platform must provide power cost tables to the
Energy Model framework in order to make EAS start. To do so, please refer to
the in-code documentation of the independent Energy Model framework in
include/linux/energy_model.h near em_register_perf_domain().

Please also note that the scheduling domains need to be re-built after the
Energy Model has been registered in order to start EAS.


  6.3 - Energy Model complexity

The task wake-up path is very latency-sensitive. When the Energy Model of a
platform is too complex (too many CPUs, too many performance domains, too many
performance states, ...), the cost of using it in the wake-up path can become
prohibitive. Indeed, the energy-aware wake-up algorithm has a complexity along
the lines of:

	C = Nd * (Nc + Ns)

with: Nd the number of performance domains; Nc the number of CPUs; and Ns the
total number of performance states (ex: for two perf. domains with 4 perf.
states each, Ns = 8).

A complexity check is performed at the root domain level, when scheduling
domains are built. EAS will not start on a root domain if its C happens to be
higher than the completely arbitrary EM_MAX_COMPLEXITY threshold (2048 at the
time of writing).

If you really want to use EAS but the complexity of your platform's Energy
Model is too high to be used with a single root domain, you're left with only
two possible options:

    1. split your system into separate, smaller, root domains using exclusive
       cpusets and enable EAS locally on each of them. This option has the
       benefit to work out of the box but the drawback of preventing load
       balance between root domains, which can result in an unbalanced system
       overall;
    2. submit patches to reduce the complexity of the EAS wake-up algorithm,
       hence enabling it to cope with larger Energy Models in reasonable time.


  6.4 - Schedutil governor

EAS tries to predict at which performance state will the CPUs be running in the
close future in order to estimate their energy consumption. To do so, it is
assumed that performance states of CPUs follow their utilization.

Although it is very difficult to provide hard guarantees regarding the accuracy
of this assumption in practice (because the hardware might not do what it is
told to do, for various reasons), schedutil as opposed to other CPUFreq
governors at least _requests_ frequencies using the utilization signals.
Consequently, the only sane governor to use together with EAS is schedutil,
because it is the only one providing some degree of consistency between
frequency requests and energy predictions.

So, using EAS with any other governor than schedutil is not supported.


  6.5 FIE / CIE

In order to make accurate prediction across CPUs and for all performance
states, EAS needs frequency-invariant and CPU-invariant PELT signals. These can
be obtained using the architecture-defined arch_scale{cpu,freq}_capacity()
callbacks.

Using EAS on a platform that doesn't implement these two callbacks is not
supported.
