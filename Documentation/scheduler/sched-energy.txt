			   =======================
			   Energy Aware Scheduling
			   =======================

1. Introduction
---------------

Energy Aware Scheduling (or EAS) gives the ability to the scheduler to predict
the impact of its decisions on the system energy. EAS relies on an Energy Model
(EM) of the platform to select an energy efficient CPU for each task, with a
minimal impact on throughput. This document aims at providing an introduction
on how EAS works, what are the main design decisions behind it, and detail what
is needed to get it to run.

Before going any further, please note that at the time of writing:

   /!\ EAS does not support platforms with symmetric CPU topologies /!\

For now, EAS operates only on platforms with heterogeneous CPU topologies (such
as Arm big.LITTLE) because this is where the potential for saving energy through
scheduling is the highest.

The actual EM used by EAS is _not_ maintained by the scheduler, but by a
dedicated framework. For details about this framework and what it provides,
please refer to its documentation, not to the scheduler's.				// XXX Ugly.


2. Energy-Aware task placement
------------------------------

EAS overloads the CFS task wake-up balancing code. It uses the EM of the
platform and the task's PELT signals to choose an energy-efficient target CPU
during wake-up balance. When EAS is enabled, select_task_rq_fair() calls
find_energy_efficient_cpu() (abbreviated feec() in the rest of the document) to
do the placement decision. feec() looks for the CPU with the highest spare
capacity in each performance domain, and checks if placing the task there could
save energy compared to leaving it on prev_cpu.

feec() uses compute_energy() to estimate what will be the energy consumed by the
system if the waking task was migrated. compute_energy() looks at the current
utilization landscape of the CPUs and adjusts it to 'simulate' the task
migration. The EM framework provides the em_pd_energy() API which computes the
expected energy consumption of each performance domain for the given utilization
landscape.

An example of task placement decision based on energy is detailed below.

Example 1.
    Let us consider a (fake) platform with an asymmetric CPU topology. The
    platform has 2 independent performance domains for CPUs, with two CPUs
    in each. CPU0 and CPU1 are little CPUs; CPU2 and CPU3 big.

    The scheduler must decide where to place a task P with 200 of util_avg.
    Its prev_cpu is CPU 0.

    The current utilization landscape of the CPUs is depicted on the graph
    below. CPUs 0 to 3 respectively have 200, 100, 600 and 500 of current
    utilization (as given by cpu_util_wake(*, P), meaning that the util_avg
    of P has been removed). Each performance domain has three Operating
    Performance Points (OPPs). The CPU capacity and power cost associated
    with each OPP is listed in the Energy Model table.

    CPU util.
      1024                 - - - - - - -              Energy Model
                                               +-----------+-------------+
                                               |  Little   |     Big     |
       768                 =============       +-----+-----+------+------+
                                               | Cap | Pwr | Cap  | Pwr  |
                                               +-----+-----+------+------+
       512  - - - - - -    - ##- - - - -       | 170 | 50  | 512  | 400  |
                             ##     ##         | 341 | 150 | 768  | 800  |
       341  ===========      ##     ##         | 512 | 300 | 1024 | 1700 |
                             ##     ##         +-----+-----+------+------+
       170  -## - - - -      ##     ##
             ##     ##       ##     ##
           ------------    -------------
            CPU0   CPU1     CPU2   CPU3

       * Current OPP: =====     * Other OPP: - - -     * 100 of util_avg: ##


    feec() will first look for the CPUs with the maximum spare capacity
    in the two performance domains. In this example, CPU1 and CPU3. Then
    It will estimate the energy of the system if P was placed on either
    of them, and check if that would save some energy compared to leaving
    P on CPU0. EAS assumes that OPPs follow utilization (which is coherent
    with the behaviour of the schedutil CPUFreq governor, see Section 5.		// Reference 'Dependencies & Req'
    for more details on this topic).

    The util_avg of P is shown on the figures below as 'PP'.

    Case 1. P stays on prev_cpu / CPU 0
    ***********************************

      1024                 - - - - - - -

                                            Energy calculation:
       768                 =============     * CPU0: 400 / 512 * 300 = 234
                                             * CPU1: 100 / 512 * 300 = 58
                                             * CPU2: 600 / 768 * 800 = 625
       512  ===========    - ##- - - - -     * CPU3: 500 / 768 * 800 = 520
                             ##     ##          => total_energy = 1437
       341  -PP - - - -      ##     ##
             PP              ##     ##
       170  -## - - - -      ##     ##
             ##     ##       ##     ##
           ------------    -------------
            CPU0   CPU1     CPU2   CPU3


    Case 2. P is migrated to CPU1
    *****************************

      1024                 - - - - - - -

                                            Energy calculation:
       768                 =============     * CPU0: 200 / 341 * 150 = 88
                                             * CPU1: 300 / 341 * 150 = 131
                                             * CPU2: 600 / 768 * 800 = 625
       512  - - - - - -    - ##- - - - -     * CPU3: 500 / 768 * 800 = 520
                             ##     ##          => total_energy = 1364
       341  ===========      ##     ##
                    PP       ##     ##
       170  -## - - PP-      ##     ##
             ##     ##       ##     ##
           ------------    -------------
            CPU0   CPU1     CPU2   CPU3


    Case 3. P is migrated to CPU3
   ******************************

      1024                 - - - - - - -

                                            Energy calculation:
       768                 =============     * CPU0: 200 / 341 * 150 = 88
                                             * CPU1: 100 / 341 * 150 = 43
                                    PP       * CPU2: 600 / 768 * 800 = 625
       512  - - - - - -    - ##- - -PP -     * CPU3: 700 / 768 * 800 = 729
                             ##     ##          => total_energy = 1485
       341  ===========      ##     ##
                             ##     ##
       170  -## - - - -      ##     ##
             ##     ##       ##     ##
           ------------    -------------
            CPU0   CPU1     CPU2   CPU3


    From these calculations, the Case 2 has the lowest total energy. So CPU 1
    is be the best candidate from an energy-efficiency standpoint.


One may wonder, given Example 1, if it ever makes sense to use the big cores for
energy reasons. The answer is yes. That can be the case in various scenarios
depending on the platforms. For some systems, the high OPPs of the little CPUs
can be less energy efficient than the lowest OPPs of the bigs. So, if the little
CPUs happen to have enough load, a small task could be better of executing on
the big side in order to save energy.

And even in the case where all OPPs of the big CPUs are less energy-efficient
than those of the little, using the big CPUs for a small task might still, under
specific conditions, save energy. Placing a task on a little CPU can result
in raising the OPP of the entire cluster, and that will increase the cost of
the tasks already running there. If the waking task is placed on a big CPU, its
own execution cost might be higher than if it was running on a little, but it
won't impact the other tasks of the little CPUs which will keep running at a
lower OPP. So, at the system level, the extra cost of running that one task on a
big core can be smaller than the cost of raising the OPP on the little CPUs for
all other tasks.

The examples above would be nearly impossible to get right in a generic way, and
for all platforms, without knowing the cost of running at different OPPs on all
CPUs of the system. Thanks to its EM-based design, EAS should cope with them
correctly without too many troubles.

In order to access the Energy Model of the platform safely and efficiently in
the wake-up path, EAS extends the topology code of the scheduler with
additional data structures alongside the scheduling domains.


3. Topology representation
--------------------------

Most of the platform knowledge used by EAS is directly read from the EM
framework, which basically provides a power cost table and a cpumask for each
performance domain in the system. The scheduler manages references to these
objects in the topology code when the scheduling domains are built, or re-built.
EAS maintains for each root domain (rd) a singly linked list of all performance
domains intersecting the rd->span. Each node in the list contains a pointer to a
struct em_perf_domain as provided by the EM framework.

The lists are attached to the root domains in order to cope with complex
scenarios including exclusive cpuset configurations. Since the boundaries of
exclusive cpusets do not not necessarily match those of performance domains, the
lists of different root domains can contain duplicate elements.

Example 2.
    Let us consider a platform with 12 CPUs, split in 3 performance domains
    (pd0, pd4 and pd8), organized as follows:

		  pd0        pd4        pd8
		0 1 2 3    4 5 6 7    8 9 10 11
		|-----RD1-----|------RD2------|

    Now, consider that userspace decided to split the system with two
    exclusive cpusets, hence creating two independent root domains, each
    containing 6 CPUs. The two root domains are denoted RD1 and RD2 in the
    above figure. Since pd4 intersects with both RD1 and RD2, it will be
    present in both lists:

		RD1->pd: pd0 -> pd4
		RD2->pd: pd4 -> pd8

    Please note that the scheduler will create two list nodes for pd4, but both
    will hold a pointer to the same em_perf_domain of the EM framework.


Both feec() and compute_energy() use these lists to traverse all performance
domains of the current rd. Since the access to these list can happen
concurrently with hotplug and other things, they are protected by RCU, like the
rest of topology structures manipulated by the scheduler.

EAS also maintains a static key (sched_energy_present) which is enabled when at
least one root domain meets all conditions for EAS to start. Those conditions
are summarized in section 5.								// Reference 'Dependencies & Req'

The additional data structures in the topology code provides EAS with the EM
safely and efficiently in the wake-up path, hence enabling energy efficient
decisions. But while EAS aims at saving energy, it also tries to do so without
harming throughput too much. One of the ways to address this is to use the
concept of over-utiliztion.


4. Over-utilization
-------------------


5. Dependencies and requirements for EAS
----------------------------------------
    a. Asymmetric CPU topology
    b. Energy Model complexity
    c. Schedutil governor
    d. FIE / CIE


6. Others
[ No idle costs]
